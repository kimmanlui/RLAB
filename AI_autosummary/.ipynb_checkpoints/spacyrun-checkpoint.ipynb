{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 11.22 <a class=\"anchor\" id=\"TOC\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Startup--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"Startup--&nbsp;&nbsp;TOC-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Startup  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#NLP-Function--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"NLP-Function--&nbsp;&nbsp;TOC-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>NLP Function  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#freqText-(text,-lem=1)---&nbsp;&nbsp;TOC\" data-toc-modified-id=\"freqText-(text,-lem=1)---&nbsp;&nbsp;TOC-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>freqText (text, lem=1)  <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#cntUniqueWordsText-(text,-lem=1)---&nbsp;&nbsp;TOC\" data-toc-modified-id=\"cntUniqueWordsText-(text,-lem=1)---&nbsp;&nbsp;TOC-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>cntUniqueWordsText (text, lem=1)  <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#getUniqueWordsText-(text,-numV=20,-lem=1)---&nbsp;&nbsp;TOC\" data-toc-modified-id=\"getUniqueWordsText-(text,-numV=20,-lem=1)---&nbsp;&nbsp;TOC-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>getUniqueWordsText (text, numV=20, lem=1)  <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#getSim,-getSimMean,-Average--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"getSim,-getSimMean,-Average--&nbsp;&nbsp;TOC-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>getSim, getSimMean, Average <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#summaryText,-cntWord,-minText,-topText--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"summaryText,-cntWord,-minText,-topText--&nbsp;&nbsp;TOC-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>summaryText, cntWord, minText, topText <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#getTS--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"getTS--&nbsp;&nbsp;TOC-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>getTS <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#cnt_sentence,-top_sentence_all,-top_sentence,-top_sentence_from_sorted_sentenece--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"cnt_sentence,-top_sentence_all,-top_sentence,-top_sentence_from_sorted_sentenece--&nbsp;&nbsp;TOC-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>cnt_sentence, top_sentence_all, top_sentence, top_sentence_from_sorted_sentenece <a class=\"anchor\"></a> &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li></ul></li><li><span><a href=\"#Email-Function--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"Email-Function--&nbsp;&nbsp;TOC-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Email Function  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#processEmailRaw,-extractHTML,-getText,-fixMinor-&nbsp;&nbsp;TOC\" data-toc-modified-id=\"processEmailRaw,-extractHTML,-getText,-fixMinor-&nbsp;&nbsp;TOC-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>processEmailRaw, extractHTML, getText, fixMinor &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#fixUniCode,-replyText,-replyTextHome-&nbsp;&nbsp;TOC\" data-toc-modified-id=\"fixUniCode,-replyText,-replyTextHome-&nbsp;&nbsp;TOC-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>fixUniCode, replyText, replyTextHome &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#getPayload,-getPayloadMsg-&nbsp;&nbsp;TOC\" data-toc-modified-id=\"getPayload,-getPayloadMsg-&nbsp;&nbsp;TOC-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>getPayload, getPayloadMsg &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#handleJunkBox-&nbsp;&nbsp;TOC\" data-toc-modified-id=\"handleJunkBox-&nbsp;&nbsp;TOC-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>handleJunkBox &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li></ul></li><li><span><a href=\"#Knowledge-Graph-Function--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"Knowledge-Graph-Function--&nbsp;&nbsp;TOC-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Knowledge Graph Function  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#get_entities,-get_relation,-getGraphInfo,-genGraph-&nbsp;&nbsp;TOC\" data-toc-modified-id=\"get_entities,-get_relation,-getGraphInfo,-genGraph-&nbsp;&nbsp;TOC-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>get_entities, get_relation, getGraphInfo, genGraph &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li></ul></li><li><span><a href=\"#Main-Program--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"Main-Program--&nbsp;&nbsp;TOC-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Main Program  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li><li><span><a href=\"#Test-Knowledge-Graph--&nbsp;&nbsp;TOC\" data-toc-modified-id=\"Test-Knowledge-Graph--&nbsp;&nbsp;TOC-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Test Knowledge Graph  &nbsp;&nbsp;<a href=\"#TOC\">TOC</a></a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Startup  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the basic information from logininfo.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "infile = open(\"/Users/admin/Documents/Dropbox/jupyter/prj/logininfo.xml\",\"r\")\n",
    "contents = infile.read()\n",
    "soup = BeautifulSoup(contents,'xml')\n",
    "\n",
    "username    =soup.conf.hotmail_username.get_text()\n",
    "password    =soup.conf.hotmail_password.get_text()\n",
    "HOTMAIL_HOST=soup.conf.hotmail_host.get_text()\n",
    "HOTMAIL_USER=username\n",
    "HOTMAIL_PASS=password\n",
    "SMTP_HOST   =soup.conf.smtp_host.get_text()\n",
    "SMTP_USER   =soup.conf.smtp_user.get_text()\n",
    "SMTP_PASS   =soup.conf.smtp_pass.get_text()\n",
    "testWho     =soup.conf.testwho.get_text()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select which engine to use spacy : 1 or nltk : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacyEng=1\n",
    "debug=0\n",
    "knowledgeGraphFlag=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Function  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freqText (text, lem=1)  <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freqText(text, lem=1):\n",
    "    import nltk \n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "    stm = nltk.wordnet.WordNetLemmatizer()\n",
    "    #stm = nltk.stem.SnowballStemmer('english')\n",
    "    #stm = nltk.stem.PorterStemmer() \n",
    "    # Tokenizing the text \n",
    "    stopWords = set(stopwords.words(\"english\")) \n",
    "    words = word_tokenize(text) \n",
    "\n",
    "    # Creating a frequency table to keep the  \n",
    "    # score of each word \n",
    "\n",
    "    freqTable = dict() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if lem==1: \n",
    "            word=stm.lemmatize(word)    \n",
    "            #word=stm.stem(word) \n",
    "        if word in stopWords: \n",
    "            continue    \n",
    "        if word in freqTable:         \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "    return(freqTable)        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cntUniqueWordsText (text, lem=1)  <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cntUniqueWordsText (text, lem=1):\n",
    "    freqTable = freqText(text, lem)\n",
    "    return(len(freqTable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getUniqueWordsText (text, numV=20, lem=1)  <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def getUniqueWordsText (text, numV=20, lem=1):\n",
    "    freqTable = freqText(text, lem)\n",
    "    #print(freqTable)\n",
    "    x = freqTable\n",
    "    #sorted_x = sorted(x.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_x = sorted(x.items(), key=itemgetter(1), reverse=True)\n",
    "    a=str(sorted_x[0:numV])\n",
    "    #print(a)\n",
    "    a=a.replace('),', ')\\n')\n",
    "    a=a.replace('[','')\n",
    "    a=a.replace(']','')\n",
    "    a=a.replace(\"('\",'')\n",
    "    a=a.replace(\"',\",' -> ')\n",
    "    a=a.replace(\")\",'')\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getSim, getSimMean, Average <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSim (sorted_x, limit):\n",
    "    sents=sorted_sentence[0:limit]\n",
    "    anaM=np.matlib.zeros((limit,limit))\n",
    "\n",
    "    doc=[]\n",
    "    for ii in range(len(sents)):\n",
    "        doc.append(nlp(str(sents[ii][0])))\n",
    "\n",
    "\n",
    "    for ii in range(0, limit) :\n",
    "        for kk in range(ii, limit):\n",
    "                simV=round(sents[kk][0].similarity(sents[ii][0]),2)\n",
    "                anaM[ii,kk]=simV\n",
    "                #print(str(ii)+' '+str(kk)+' '+str(simV))\n",
    "\n",
    "    return(anaM.round(2))   \n",
    "\n",
    "def getSimMean(sorted_x, limit):\n",
    "    sents=sorted_sentence[0:limit]\n",
    "    anaM=np.matlib.zeros((limit,limit))\n",
    "\n",
    "    val=[]\n",
    "    doc=[]\n",
    "    for ii in range(len(sents)):\n",
    "        doc.append(nlp(str(sents[ii][0])))\n",
    "\n",
    "\n",
    "    for ii in range(0, limit) :\n",
    "        for kk in range(ii, limit):\n",
    "                simV=round(sents[kk][0].similarity(sents[ii][0]),2)\n",
    "                val.append(simV)\n",
    "                #print(str(ii)+' '+str(kk)+' '+str(simV))\n",
    "\n",
    "    return(val)    \n",
    "\n",
    "def Average(lst): \n",
    "    return round(sum(lst) / len(lst),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summaryText, cntWord, minText, topText <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# REF : https://www.geeksforgeeks.org/python-text-summarizer/\n",
    "def summaryText(text, threshold=1.2):\n",
    "    import nltk \n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "    # Tokenizing the text \n",
    "    stopWords = set(stopwords.words(\"english\")) \n",
    "    words = word_tokenize(text) \n",
    "\n",
    "    # Creating a frequency table to keep the  \n",
    "    # score of each word \n",
    "\n",
    "    freqTable = dict() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if word in stopWords: \n",
    "            continue\n",
    "        if word in freqTable: \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # Creating a dictionary to keep the score \n",
    "    # of each sentence \n",
    "    sentences = sent_tokenize(text) \n",
    "    sentenceValue = dict() \n",
    "\n",
    "    for sentence in sentences: \n",
    "        for word, freq in freqTable.items(): \n",
    "            if word in sentence.lower(): \n",
    "                if sentence in sentenceValue: \n",
    "                    sentenceValue[sentence] += freq \n",
    "                else: \n",
    "                    sentenceValue[sentence] = freq \n",
    "\n",
    "\n",
    "\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue: \n",
    "        sumValues += sentenceValue[sentence] \n",
    "   \n",
    "    # Average value of a sentence from the original text \n",
    "\n",
    "    average = int(sumValues / len(sentenceValue)) \n",
    "\n",
    "    # Storing sentences into our summary. \n",
    "\n",
    "    \n",
    "   \n",
    "    summary = '' \n",
    "    for sentence in sentences: \n",
    "        if (sentence in sentenceValue) and (sentenceValue[sentence] > (threshold * average)): \n",
    "            summary += \" \" + sentence \n",
    "    return(summary)\n",
    "    \n",
    "def cntWord(test_string):\n",
    "  import string  \n",
    "  res = sum([i.strip(string.punctuation).isalpha() for i in test_string.split()])\n",
    "  return(res)\n",
    "\n",
    "def minText(text):\n",
    "    startAvgDex=0.9\n",
    "    startAvgDexText=summaryText(text, startAvgDex)\n",
    "    startAvgWordCnt=cntWord(startAvgDexText)\n",
    "    lastAvgWordCnt=startAvgWordCnt\n",
    "    lastAvgDexText=startAvgDexText\n",
    "\n",
    "    for x in range(6):\n",
    "        startAvgDex=startAvgDex + 0.1\n",
    "        startAvgDexText=summaryText(text, startAvgDex)\n",
    "        startAvgWordCnt=cntWord(startAvgDexText)\n",
    "        if startAvgWordCnt==0 :\n",
    "            minAvgDex=lastAvgWordCnt\n",
    "        else :\n",
    "            lastAvgWordCnt=startAvgWordCnt\n",
    "            lastAvgDexText=startAvgDexText  \n",
    "    return(lastAvgDexText)\n",
    "\n",
    "def topText(text, top=2):\n",
    "    import nltk \n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "    # Tokenizing the text \n",
    "    stopWords = set(stopwords.words(\"english\")) \n",
    "    words = word_tokenize(text) \n",
    "\n",
    "    # Creating a frequency table to keep the  \n",
    "    # score of each word \n",
    "\n",
    "    freqTable = dict() \n",
    "    for word in words: \n",
    "        word = word.lower() \n",
    "        if word in stopWords: \n",
    "            continue\n",
    "        if word in freqTable: \n",
    "            freqTable[word] += 1\n",
    "        else: \n",
    "            freqTable[word] = 1\n",
    "\n",
    "    # Creating a dictionary to keep the score \n",
    "    # of each sentence \n",
    "    sentences = sent_tokenize(text) \n",
    "    sentenceValue = dict() \n",
    "\n",
    "    for sentence in sentences: \n",
    "        for word, freq in freqTable.items(): \n",
    "            if word in sentence.lower(): \n",
    "                if sentence in sentenceValue: \n",
    "                    sentenceValue[sentence] += freq \n",
    "                else: \n",
    "                    sentenceValue[sentence] = freq \n",
    "\n",
    "\n",
    "\n",
    "    sumValues = 0\n",
    "    for sentence in sentenceValue: \n",
    "        sumValues += sentenceValue[sentence] \n",
    "   \n",
    "    # Average value of a sentence from the original text \n",
    "\n",
    "    average = int(sumValues / len(sentenceValue)) \n",
    "\n",
    "    # Storing sentences into our summary. \n",
    "    sentenceValueA=dict(sorted(sentenceValue.items(), key=lambda item: item[1], reverse=True))\n",
    "    i=0\n",
    "    summary=''\n",
    "    for key, value in sentenceValue.items():\n",
    "        temp = [key,value]\n",
    "        #print(key)\n",
    "        #print(value)\n",
    "        if i<top:\n",
    "            summary +=key+\"\\n\\n\"\n",
    "        i+=1  \n",
    "    return(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getTS <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import re\n",
    "def getTS(text):\n",
    "    gotText=''\n",
    "    result = re.search(r\".s\\d.\", text)\n",
    "    if not (result):\n",
    "        gotText='2'\n",
    "    else :\n",
    "        gotText=result.group(0)\n",
    "        gotText=gotText.replace('.', '')\n",
    "        gotText=gotText.replace('s', '')\n",
    "    iText = int(gotText)\n",
    "    if iText <= 1 :\n",
    "        iText = 1\n",
    "    if iText >= 20 :\n",
    "        iText = 20\n",
    "    return(iText)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U pip setuptools wheel\n",
    "#!pip3 install -U spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spacyEng==1:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    import en_core_web_lg\n",
    "    nlp = en_core_web_lg.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnt_sentence, top_sentence_all, top_sentence, top_sentence_from_sorted_sentenece <a class=\"anchor\" ></a> &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_sentence(text):\n",
    "    keyword = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "        \n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    \n",
    "    summary = []\n",
    "    \n",
    "    sorted_x = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True)    \n",
    "    return(len(sorted_x))\n",
    "\n",
    "\n",
    "def top_sentence_all(text):\n",
    "    keyword = []\n",
    "    pos_tag = ['PROPN', 'ADJ', 'NOUN', 'VERB']\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        if(token.text in nlp.Defaults.stop_words or token.text in punctuation):\n",
    "            continue\n",
    "        if(token.pos_ in pos_tag):\n",
    "            keyword.append(token.text)\n",
    "    \n",
    "    freq_word = Counter(keyword)\n",
    "    max_freq = Counter(keyword).most_common(1)[0][1]\n",
    "    for w in freq_word:\n",
    "        freq_word[w] = (freq_word[w]/max_freq)\n",
    "        \n",
    "    sent_strength={}\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.text in freq_word.keys():\n",
    "                if sent in sent_strength.keys():\n",
    "                    sent_strength[sent]+=freq_word[word.text]\n",
    "                else:\n",
    "                    sent_strength[sent]=freq_word[word.text]\n",
    "    sorted_sentence = sorted(sent_strength.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    return(sorted_sentence)\n",
    "\n",
    "\n",
    "\n",
    "def top_sentence(text, limit):\n",
    "    sorted_x=top_sentence_all(text)\n",
    "    summary = []\n",
    "    counter = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        summary.append(\"\\n\\n\"+str(sorted_x[i][0]).capitalize())\n",
    "\n",
    "        counter += 1\n",
    "        if(counter >= limit):\n",
    "            break            \n",
    "    return ' '.join(summary)\n",
    "\n",
    "\n",
    "\n",
    "def top_sentence_from_sorted_sentenece(sorted_x, limit):\n",
    "    summary = []\n",
    "    counter = 0\n",
    "    for i in range(len(sorted_x)):\n",
    "        summary.append(\"\\n\\n\"+str(i+1)+\" \"+str(sorted_x[i][0]).capitalize())\n",
    "\n",
    "        counter += 1\n",
    "        if(counter >= limit):\n",
    "            break            \n",
    "    return ' '.join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text - to summarize  \n",
    "text_1 = \"\"\" \n",
    "There are many techniques available to generate extractive summarization to keep it simple, I will be using an unsupervised learning approach to find the sentences similarity and rank them. Summarization can be defined as a task of producing a concise and fluent summary while preserving key information and overall meaning. One benefit of this will be, you don’t need to train and build a model prior start using it for your project. It’s good to understand Cosine similarity to make the best use of the code you are going to see. Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. Its measures cosine of the angle between vectors. The angle will be 0 if sentences are similar.\n",
    "\"\"\"\n",
    "text_2 = '''Yamaha is reminding people that musical equipment cases are for musical equipment — not people — two weeks after fugitive auto titan Carlos Ghosn reportedly was smuggled out of Japan in one. In a tweet over the weekend, the Japanese musical equipment company said it was not naming any names, but noted there had been many recent stories about people getting into musical equipment cases. Yamaha (YAMCY) warned people not to get into, or let others get into, its cases to avoid \"unfortunate accidents.\" Multiple media outlets have reported that Ghosn managed to sneak through a Japanese airport to a private jet that whisked him out of the country by hiding in a large, black music equipment case with breathing holes drilled in the bottom. CNN Business has not independently confirmed those details of his escape. The former Nissan (NSANF) CEO had been out on bail awaiting trial in Japan on charges of financial wrongdoing before making his stunning escape to Lebanon at the end of December. Ghosn has referred to his departure as an effort to \"escape injustice.\" In an interview with CNN\\'s Richard Quest last week, Ghosn did not comment on the nature of his escape, saying he didn\\'t want to endanger any of the people who aided in the operation. Ghosn did, however, respond to a question about what it felt like to ride through the airport in a packing case by first declining to comment but then adding: \"Freedom, no matter the way it happens, is always sweet.\" In a press conference in Lebanon ahead of the CNN interview last Wednesday, Ghosn\\'s first public appearance since fleeing Japan, Ghosn said he decided to leave the country because he believed he would not receive a fair trial, a claim Japanese authorities have disputed. Brands sometimes capitalize on their tangential relationship to big news in order to attract attention on social media. Yamaha is one of Japan\\'s best known brands and Ghosn was one of Japan\\'s top executives before being ousted from Nissan — a match made in social media heaven. Not surprisingly, Yamaha\\'s post went viral on Twitter over the weekend.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Number of Sentences: '+str(cnt_sentence(text_3)))\n",
    "#print(top_sentence(text_2, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REF https://www.devdungeon.com/content/read-and-send-email-python#toc-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Function  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processEmailRaw, extractHTML, getText, fixMinor &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmailRaw(raw_email):\n",
    "    bodyAll=''\n",
    "    raw_email_string = raw_email.decode('utf-8')\n",
    "    # converts byte literal to string removing b''\n",
    "    email_message = email.message_from_string(raw_email_string)\n",
    "    # this will loop through all the available multiparts in mail\n",
    "    for part in email_message.walk():\n",
    "     if part.get_content_type() == \"text/plain\": # ignore attachments/html\n",
    "      body = part.get_payload(decode=True)  \n",
    "      bodyAll=bodyAll+body.decode('utf-8')\n",
    "     else:\n",
    "      continue\n",
    "    return(bodyAll)\n",
    "\n",
    "def extractHTML(sbody):\n",
    "    stDex=sbody.find('<html>')\n",
    "    enDex=sbody.find('</html>')\n",
    "    sbody2=sbody[stDex:(enDex+7)]\n",
    "    return(sbody2)\n",
    "\n",
    "def getText(sbody2):\n",
    "    from bs4 import BeautifulSoup\n",
    "    #soup = BeautifulSoup(bodyAll, \"lxml\")\n",
    "    soup = BeautifulSoup(sbody2, features=\"html.parser\")\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "\n",
    "    return(text)\n",
    "\n",
    "def fixMinor(text):\n",
    "    text=text.replace(\"=A1=A6\", \"\")\n",
    "    text=text.replace(\"=A1=A7\", \"\")\n",
    "    text=text.replace(\"=A1=A8\", \"\")\n",
    "    text=text.replace(\"=92\", \"\")\n",
    "    text=text.replace(\"=\\n\", \"\")\n",
    "    text=text.replace(\"\\n\", \" \")\n",
    "    text=text.replace(\"\\xa0\",\" \")\n",
    "    return(text)\n",
    "#print(bodyAll)\n",
    "#http://www.vineetdhanawat.com/blog/2012/06/how-to-extract-email-gmail-contents-as-text-using-imaplib-via-imap-in-python-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixUniCode, replyText, replyTextHome &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/46160886/how-to-send-smtp-email-for-office365-with-python-using-tls-ssl\n",
    "def fixUnicode(text):\n",
    "    text = text.replace('\\u2013','-')\n",
    "    text = text.replace('\\u2014','-')\n",
    "    text = text.replace('\\u201c','\"')\n",
    "    text = text.replace('\\u201d','\"')\n",
    "    text = text.replace('\\u2018','\\'')\n",
    "    text = text.replace('\\u2019','\\'')\n",
    "    return(text)\n",
    "\n",
    "def replyText(fromWho, toWho, subject, text, SMTP_HOST, SMTP_USER,SMTP_PASS):\n",
    "    text=fixUnicode(text)\n",
    "    import smtplib\n",
    "    SMTP_SSL_PORT=587\n",
    "    # Craft the email by hand\n",
    "    from_email = toWho  # or simply the email address\n",
    "    to_emails = [fromWho]\n",
    "    body = text\n",
    "    headers = f\"From: {from_email}\\r\\n\"\n",
    "    headers += f\"To: {', '.join(to_emails)}\\r\\n\" \n",
    "    headers += f\"Subject:\"+subject+\"\\r\\n\"\n",
    "    email_message = headers + \"\\r\\n\" + body  # Blank line needed between headers and body\n",
    "    mailserver = smtplib.SMTP(SMTP_HOST,SMTP_SSL_PORT)\n",
    "    mailserver.ehlo()\n",
    "    mailserver.starttls()\n",
    "    mailserver.login(SMTP_USER, SMTP_PASS)\n",
    "    mailserver.sendmail(from_email,to_emails,email_message)\n",
    "    mailserver.quit()\n",
    "    \n",
    "\n",
    "\n",
    "def replyTextHome(fromWho, toWho, subject, text, SMTP_HOST, SMTP_USER, SMTP_PASS):\n",
    "    text=fixUnicode(text)\n",
    "    import smtplib\n",
    "    SMTP_SSL_PORT=25\n",
    "    # Craft the email by hand\n",
    "    from_email = toWho  # or simply the email address\n",
    "    to_emails = [fromWho]\n",
    "    body = text\n",
    "    headers = f\"From: {from_email}\\r\\n\"\n",
    "    headers += f\"To: {', '.join(to_emails)}\\r\\n\" \n",
    "    headers += f\"Subject:\"+subject+\"\\r\\n\"\n",
    "    email_message = headers + \"\\r\\n\" + body  # Blank line needed between headers and body\n",
    "\n",
    "    mailserver = smtplib.SMTP(SMTP_HOST,SMTP_SSL_PORT)\n",
    "    mailserver.ehlo()\n",
    "    #mailserver.starttls()\n",
    "    mailserver.login(SMTP_USER, SMTP_PASS)\n",
    "    mailserver.sendmail(from_email,to_emails,email_message)\n",
    "    mailserver.quit()    \n",
    "\n",
    "def waitMin(mins):\n",
    "    time.sleep(mins*60)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getPayload, getPayloadMsg &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPayload(raw_email):\n",
    "    msg = email.message_from_bytes(raw_email)\n",
    "    if msg.is_multipart():\n",
    "        # iterate over email parts\n",
    "        for part in msg.walk():\n",
    "            # extract content type of email\n",
    "            content_type = part.get_content_type()\n",
    "            content_disposition = str(part.get(\"Content-Disposition\"))\n",
    "            try:\n",
    "                # get the email body\n",
    "                body = part.get_payload(decode=True).decode()\n",
    "            except:\n",
    "                pass\n",
    "            if content_type == \"text/plain\" and \"attachment\" not in content_disposition:\n",
    "                # print text/plain emails and skip attachments\n",
    "                body=body\n",
    "    else:\n",
    "        # extract content type of email\n",
    "        content_type = msg.get_content_type()\n",
    "        # get the email body\n",
    "        body = msg.get_payload(decode=True).decode()\n",
    "        if content_type == \"text/plain\":\n",
    "            # print only text email parts\n",
    "            body=body\n",
    "    return(body)\n",
    "\n",
    "def getPayloadMsg(body):\n",
    "    htoken=body[0:body.find('\\r\\n')]\n",
    "    s=body.find('\\r\\n\\r\\n')\n",
    "    body1=body[s+4:len(body)] \n",
    "    e=body1.find(htoken)\n",
    "    body2=body1[0:e]\n",
    "    base64_message = body2\n",
    "    base64_bytes = base64_message.encode('utf-8')\n",
    "    message_bytes = base64.b64decode(base64_bytes)\n",
    "    message = message_bytes.decode('utf-8')\n",
    "    return(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handleJunkBox &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleJunkBox():\n",
    "    statusj, countj = M.select('junk')\n",
    "    _, message_numbers_raw = M.search(None, 'ALL')\n",
    "    junkMsg=message_numbers_raw[0].split()\n",
    "    if len(junkMsg)>0:\n",
    "        print(\"Moving Junk Msg \"+str(len(junkMsg)))\n",
    "        for i in range(len(junkMsg)):\n",
    "            msg_uid=junkMsg[0]\n",
    "            M.store(msg_uid, '-FLAGS', '\\\\SEEN') \n",
    "            result = M.copy( msg_uid, 'Inbox')\n",
    "            #print(result[0])\n",
    "            if result[0] == 'OK':\n",
    "                mov, data = M.store( msg_uid , '+FLAGS', '(\\\\Deleted)')\n",
    "                M.expunge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS4,proxy_ip,proxy_port,True)\n",
    "#socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Function  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_entities, get_relation, getGraphInfo, genGraph &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # Matcher class object \n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  #define the pattern \n",
    "  pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "  matcher.add(\"matching_1\", None, pattern) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)\n",
    "\n",
    "def getGraphInfo(candidate_sentences):\n",
    "    entity_pairs = []\n",
    "    for i in (candidate_sentences[1:10]):\n",
    "        i=str(i)\n",
    "        entity_pairs.append(get_entities(i))\n",
    "    relations = []\n",
    "    for i in (candidate_sentences[1:10]):\n",
    "        i=str(i)\n",
    "        relations.append(get_relation(i))\n",
    "    \n",
    "    # extract subject\n",
    "    source = [i[0] for i in entity_pairs]\n",
    "\n",
    "    # extract object\n",
    "    target = [i[1] for i in entity_pairs]\n",
    "\n",
    "    kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "    return(kg_df)\n",
    "\n",
    "def genGraph(filename, kg_df):\n",
    "    G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "    plt.figure(figsize=(12,12))\n",
    "\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('OK', [b'LOGIN completed.'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import imaplib\n",
    "import email\n",
    "from datetime import datetime\n",
    "#from langdetect import detect\n",
    "import re\n",
    "import base64\n",
    "import traceback \n",
    "\n",
    "mail_server = HOTMAIL_HOST\n",
    "M=imaplib.IMAP4_SSL(host=mail_server)\n",
    "M.login(username,password)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring junk mails back to inbox\n",
    "handleJunkBox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Unseen Message\n",
    "status, count   = M.select('inbox')\n",
    "_, message_numbers_raw = M.search(None, '(UNSEEN)')\n",
    "unseenMsg=message_numbers_raw[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messageNumber=unseenMsg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(unseenMsg)\n",
    "#_, msg = M.fetch(messageNumber, '(RFC822)')    \n",
    "#message = email.message_from_bytes(msg[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fromWho=str(message[\"from\"])\n",
    "#toWho=str(message[\"to\"])\n",
    "#subject=str(message[\"subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#status, data = M.fetch(messageNumber,'(UID BODY[TEXT])')\n",
    "#raw_email=data[0][1]\n",
    "#tmpText=getText(extractHTML(processEmailRaw(data[0][1])))\n",
    "#text=fixMinor(tmpText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug :0\n"
     ]
    }
   ],
   "source": [
    "print(\"debug :\"+str(debug+0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait (13:24)\n"
     ]
    }
   ],
   "source": [
    "fromWho='X'\n",
    "subject='X'\n",
    "\n",
    "showNumFreqWord=100\n",
    "\n",
    "\n",
    "#for loopdx in range(5):\n",
    "while 1==1:\n",
    "    try:  \n",
    "        for messageNumber in unseenMsg:\n",
    "            _, msg = M.fetch(messageNumber, '(RFC822)')    \n",
    "            message = email.message_from_bytes(msg[0][1])\n",
    "            M.store(messageNumber, '+FLAGS', '\\SEEN') \n",
    "            fromWho=str(message[\"from\"])\n",
    "            if \"noreply\" not in fromWho:\n",
    "                toWho=str(message[\"to\"])\n",
    "                subject=str(message[\"subject\"])\n",
    "               \n",
    "                status, data = M.fetch(messageNumber,'(UID BODY[TEXT])')\n",
    "                raw_email=data[0][1]\n",
    "                tmpText=getText(extractHTML(processEmailRaw(data[0][1])))\n",
    "                text=fixMinor(tmpText)\n",
    "                #detectLang=detect(text)\n",
    "                \n",
    "                numTop=int(getTS(subject))\n",
    "                #numTop=2\n",
    "                \n",
    "                if len(text)!=0:                \n",
    "                    properTextRate=round(text.count('=')/len(text),4)\n",
    "                    isProperText=(properTextRate<=0.05)\n",
    "                    print(\"new message (\"+str(properTextRate)+\"): \"+fromWho+\", \"+subject)\n",
    "                    #old block here before\n",
    "                else: \n",
    "                    print('not html')\n",
    "                    bodynon=getPayload(msg[0][1])\n",
    "                    if len(bodynon)!=0:\n",
    "                        if bodynon.find('<html>')!=-1:\n",
    "                            tmpText=getText(extractHTML(bodynon))\n",
    "                            text=fixMinor(tmpText)\n",
    "                        else:\n",
    "                            tmpText=getText(bodynon)\n",
    "                            text=fixMinor(tmpText)\n",
    "                    else :\n",
    "                        text=''\n",
    "                        \n",
    "                    isProperText=(len(text)!=0)    \n",
    "                    #if len(text)!=0:\n",
    "                    \n",
    "                    \n",
    "                if isProperText:\n",
    "                    sumInfo=''\n",
    "                    orgtext=text\n",
    "                    if fromWho=='Lui KM <cskmlui@hotmail.com>':\n",
    "                        text=text.replace('(CNN)','.')\n",
    "                        \n",
    "                    \n",
    "                    #dup\n",
    "                    cntAll=cntUniqueWordsText(text)\n",
    "                    anaAll=getUniqueWordsText(text, showNumFreqWord )\n",
    "                    oCnt=cntWord(text) \n",
    "                    mRes=\"NA\"\n",
    "                    meanV=0\n",
    "                    simNum=15\n",
    "                    if spacyEng==1:\n",
    "                        sorted_sentence = top_sentence_all(text)\n",
    "                        text   = top_sentence_from_sorted_sentenece(sorted_sentence, numTop)\n",
    "                        simM   = str(getSim(sorted_sentence, simNum))\n",
    "                        meanV  = Average(getSimMean(sorted_sentence,simNum))\n",
    "                        sCnt   = cntWord(text)\n",
    "                        sumInfo= \"(\"+str(sCnt)+\"/\"+str(oCnt)+\"/\"+str(cntAll)+\"|\"+str(meanV)+\")\"\n",
    "                    else:\n",
    "                        text=topText(text)\n",
    "                        sCnt=cntWord(text)\n",
    "                        sumInfo=\"(\"+str(sCnt)+\"/\"+str(oCnt)+\"/\"+str(cntAll)+\")\"\n",
    "                    if numTop==20:\n",
    "                        text=text+\"\\r\\n\\r\\n-----Word Freq Cnt-----\\r\\n\"+anaAll\n",
    "                        text=text+\"\\r\\n\\r\\n-----Similarity Matrix-----\\r\\n\"+simM\n",
    "                    replySubject=\"[Summary \"+sumInfo+\"] \"+subject\n",
    "                        # dup                          \n",
    "                else :\n",
    "                    text=\"The received email was not an HTML formatted email.\"\n",
    "                    replySubject=\"[Problem] \"+subject\n",
    "                        \n",
    "                try:\n",
    "                    print(\"send email by replyText: \"+replySubject)\n",
    "                    if debug!=1:\n",
    "                        replyText(fromWho, toWho, replySubject , text, HOTMAIL_HOST, HOTMAIL_USER, HOTMAIL_PASS)\n",
    "                except:\n",
    "                    print(\"send email by replyTextHome: \"+replySubject)\n",
    "                    if debug!=1: \n",
    "                        replyTextHome(fromWho, toWho, replySubject , text, SMTP_HOST, SMTP_USER, SMTP_PASS)                \n",
    "            \n",
    "\n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H:%M\")    \n",
    "        print(\"wait (\"+current_time+\")\")    \n",
    "        if debug==1:\n",
    "            break\n",
    "        waitMin(1)  \n",
    "        fromWho='X'\n",
    "        subject='X'\n",
    "        #handleJunkBox\n",
    "        handleJunkBox()\n",
    "        #get Unseen Message\n",
    "        status, count   = M.select('inbox')\n",
    "        _, message_numbers_raw = M.search(None, '(UNSEEN)')\n",
    "        unseenMsg=message_numbers_raw[0].split()  \n",
    "    except:\n",
    "        traceback.print_exc() \n",
    "        print(\"Exception occurred\")\n",
    "        if fromWho!='X':\n",
    "            print(\"problem with this email:\"+fromWho+\", \"+subject) \n",
    "        waitMin(1)\n",
    "        M=imaplib.IMAP4_SSL(host=mail_server)\n",
    "        M.login(username,password)\n",
    "        status, count = M.select('inbox')\n",
    "        print(\"Exception End\")\n",
    "\n",
    "    if debug==1: \n",
    "        break\n",
    "\n",
    "print(\"-END-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.close()\n",
    "#M.logout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Knowledge Graph  &nbsp;&nbsp;[TOC](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidate_sentences=[]\n",
    "#for i in range(len(sorted_sentence)): candidate_sentences.append(sorted_sentence[i][0])\n",
    "\n",
    "    \n",
    "#for i in reversed(range(len(candidate_sentences))):\n",
    "#    if cntWord(candidate_sentences[i])<=2:\n",
    "#        del candidate_sentences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#kg_df=getGraphInfo(candidate_sentences[1:50])\n",
    "#genGraph('/Users/admin/Documents/kg_img/abc.png', kg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
